
img = cv2.VideoCapture(0)
net = cv2.dnn.readNetFromONNX('qatarlpr.onnx')
file = open('coco.txt', 'r')
classes = file.read().split('\n')

while True:
    ret, frame = img.read()

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

    blob = cv2.dnn.blobFromImage(frame, scalefactor=1 / 255, size=(640, 640), mean=[0, 0, 0], swapRB=True, crop=False)
    net.setInput(blob)
    detections = net.forward()[0]

    classes_ids = []
    confidences = []
    boxes = []
    rows = detections.shape[0]

    frame_width, frame_height = frame.shape[1], frame.shape[0]
    x_scale = frame_width / 640
    y_scale = frame_height / 640

    for i in range(rows):
        row = detections[i]
        confidence = row[4]
        if confidence > 0.5:  # Use np.any() to check if any element satisfies the condition
            classes_score = row[5:]
            ind = np.argmax(classes_score)
            if classes_score[ind] > 0.5:
                classes_ids.append(ind)
                confidences.append(confidence)
                cx, cy, w, h = row[:4]
                x1 = int((cx - w / 2) * x_scale)
                y1 = int((cy - h / 2) * y_scale)
                width = int(w * x_scale)
                height = int(h * y_scale)
                box = np.array([x1, y1, width, height])
                boxes.append(box)

    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.2, 0.2)

    for i in indices:
        x1, y1, w, h = boxes[i]
        label = classes[classes_ids[i]]
        conf = confidences[i]
        text = label + "{:.2f}".format(conf)
        cv2.rectangle(frame, (x1, y1), (x1 + w, y1 + h), (255, 0, 0), 2)
        cv2.putText(frame, text, (x1, y1 - 2), cv2.FONT_HERSHEY_COMPLEX, 0.7, (255, 0, 255), 2)

    cv2.imshow('frame', frame)

img.release()
cv2.destroyAllWindows()


# CODE
# https://www.youtube.com/watch?v=B5ganPjMOAY&ab_channel=AISearch
# https://labelstud.io/guide/quick_start
